import gc
import os
from copy import deepcopy
import torch
from torch import nn
from accelerate.logging import get_logger
from transformers import (
    SchedulerType,
    MODEL_MAPPING,
    OPTForCausalLM,
    GPT2LMHeadModel,
    BloomForCausalLM,
    ViTForImageClassification,
)
from offsite_tuning.models.clip_vit import CLIPViTForImageClassification
from offsite_tuning.models.eva_vit import EVAViTForImageClassification

import argparse


MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


logger = get_logger(__name__)


class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, activation=nn.ReLU):
        #å°†ä¼ å…¥çš„å‚æ•°èµ‹å€¼ç»™ç›¸åº”çš„å®ä¾‹å˜é‡ã€‚
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.activation = activation()
        
        #åˆ›å»ºäº†ä¸€ä¸ª`nn.ModuleList()`ç±»å‹çš„`layers`å˜é‡ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹çš„å„ä¸ªå±‚ã€‚
        self.layers = nn.ModuleList()
        #é€šè¿‡`nn.Linear`å‡½æ•°å°†è¾“å…¥å±‚è¿æ¥åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°`layers`ä¸­ã€‚
        self.layers.append(nn.Linear(input_dim, hidden_dim))
        #ä½¿ç”¨å¾ªç¯å°†å‰©ä½™çš„éšè—å±‚æ·»åŠ åˆ°`layers`ä¸­ã€‚
        for i in range(num_layers - 1):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
        #å°†æœ€åä¸€ä¸ªéšè—å±‚è¿æ¥åˆ°è¾“å‡ºå±‚ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°`layers`ä¸­ã€‚
        self.layers.append(nn.Linear(hidden_dim, output_dim))

    #æ¥å—è¾“å…¥`x`ä½œä¸ºå‚æ•°ï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡éå†`layers`åˆ—è¡¨ï¼Œå°†è¾“å…¥`x`ä¾æ¬¡ä¼ é€’ç»™æ¯ä¸ªçº¿æ€§å±‚å’Œæ¿€æ´»å‡½æ•°è¿›è¡Œè®¡ç®—ã€‚
    # æœ€åï¼Œå°†æœ€åä¸€å±‚çš„è¾“å‡ºä½œä¸ºæ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶è¿”å›å®ƒã€‚
    def forward(self, x):
        for i in range(self.num_layers):
            x = self.layers[i](x)
            x = self.activation(x)
        x = self.layers[-1](x)
        return x

#`add_prologue`å’Œ`add_epilogue`ï¼Œç”¨äºåœ¨ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ·»åŠ å‰å¯¼æ“ä½œå’Œåç»§æ“ä½œã€‚
#ä¸¤ä¸ªå‡½æ•°å¯ä»¥ç”¨äºåœ¨ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ·»åŠ é¢å¤–çš„æ“ä½œï¼Œä¾‹å¦‚æ•°æ®é¢„å¤„ç†ã€æ•°æ®è½¬æ¢æˆ–ç‰¹å¾æå–ç­‰ã€‚
def add_prologue(module, prologue):
    #é¦–å…ˆä¿å­˜äº†`module`çš„åŸå§‹å‰å‘ä¼ æ’­å‡½æ•°`module.forward`å’Œä¼ å…¥çš„`prologue`å‡½æ•°ã€‚
    module.old_forward = module.forward
    module.prologue = prologue

    #å®šä¹‰äº†ä¸€ä¸ªæ–°çš„å‰å‘ä¼ æ’­å‡½æ•°`new_forward`
    def new_forward(self):
        #æ–°çš„åŒ¿åå‡½æ•°`lambda_forward`ï¼Œè¯¥å‡½æ•°æ¥å—`*args`å’Œ`**kwargs`ï¼Œ
        #å³ä»»æ„æ•°é‡çš„ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°ã€‚åœ¨`lambda_forward`å‡½æ•°ä¸­ï¼Œ
        #é¦–å…ˆä¿å­˜äº†è¾“å…¥å‚æ•°`args`å’Œ`kwargs`åˆ°`self.input_args`å’Œ`self.input_kwargs`ä¸­ã€‚
        def lambda_forward(*args, **kwargs):
            self.input_args = args
            self.input_kwargs = kwargs
            #æ ¹æ®`prologue`å‡½æ•°æ˜¯å¦ä¸º`None`ï¼Œå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå‰å¯¼æ“ä½œã€‚
            #å¦‚æœ`prologue`ä¸ä¸º`None`ï¼Œåˆ™å°†è¾“å…¥æ•°æ®çš„ç¬¬ä¸€ä¸ªå…ƒç´ `args[0]`ä¼ é€’ç»™`prologue`å‡½æ•°è¿›è¡Œæ“ä½œï¼Œå¹¶å°†ç»“æœèµ‹ç»™å˜é‡`x`ï¼›
            #å¦åˆ™ï¼Œå°†`args[0]`èµ‹ç»™å˜é‡`x`ã€‚
            if self.prologue is not None:
                x = self.prologue(args[0])
            else:
                x = args[0]
            #å°†`x`ä¸`args[1:]`æ‹¼æ¥æˆæ–°çš„è¾“å…¥å‚æ•°`args`
            args = (x,) + args[1:]
            #è°ƒç”¨åŸå§‹çš„å‰å‘ä¼ æ’­å‡½æ•°`self.old_forward`ï¼Œä¼ å…¥æ–°çš„å‚æ•°`args`å’Œ`kwargs`
            return self.old_forward(*args, **kwargs)
        return lambda_forward
    #å°†`lambda_forward`å‡½æ•°èµ‹å€¼ç»™`module.forward`ï¼Œå®Œæˆå¯¹æ¨¡å‹çš„å‰å‘ä¼ æ’­å‡½æ•°çš„æ›¿æ¢ï¼Œå¹¶è¿”å›ä¿®æ”¹åçš„`module`ã€‚
    module.forward = new_forward(module)
    return module

def add_epilogue(module, epilogue):
    module.old_forward = module.forward
    module.epilogue = epilogue

    def new_forward(self):
        def lambda_forward(*args, **kwargs):
            output = self.old_forward(*args, **kwargs)
            #åˆ¤æ–­`output`æ˜¯å¦ä¸ºå…ƒç»„ç±»å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å°†å…¶ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ èµ‹å€¼ç»™å˜é‡`x`ï¼›å¦åˆ™ï¼Œå°†`output`èµ‹å€¼ç»™`x`ã€‚
            if isinstance(output, tuple):
                x = output[0]
            else:
                x = output

            if self.epilogue is not None:
                x = self.epilogue(x)
            #å¦‚æœ`output`æ˜¯å…ƒç»„ç±»å‹ï¼Œåˆ™å°†`x`ä¸`output[1:]`æ‹¼æ¥æˆæ–°çš„è¾“å‡ºå…ƒç»„ï¼›å¦åˆ™ï¼Œå°†`x`èµ‹å€¼ç»™`output`ã€‚
            if isinstance(output, tuple):
                output = (x,) + output[1:]
            else:
                output = x

            self.cached_output = x
            return output
        return lambda_forward
    module.forward = new_forward(module)
    return module

#`layers`æ˜¯ä¸€ä¸ª`nn.ModuleList`ç±»å‹çš„åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ç¥ç»ç½‘ç»œçš„å±‚ï¼›
#`num_student_layers`æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºè¦é€‰æ‹©çš„å­¦ç”Ÿæ¨¡å‹çš„å±‚æ•°ã€‚
def uniform_choose_layers(layers: nn.ModuleList, num_student_layers=None):
    #å¦‚æœ`num_student_layers`ä¸º`None`ï¼Œåˆ™é»˜è®¤é€‰æ‹©å’ŒåŸå§‹æ¨¡å‹ä¸€æ ·å¤šçš„å±‚æ•°ã€‚
    if num_student_layers is None:
        num_student_layers = len(layers)
    #å®šä¹‰äº†ä¸€ä¸ªç©ºçš„`nn.ModuleList`ç±»å‹çš„åˆ—è¡¨`student`ï¼Œç”¨äºå­˜å‚¨é€‰å‡ºæ¥çš„å­¦ç”Ÿæ¨¡å‹çš„å±‚ã€‚
    student = nn.ModuleList()
    #è®¡ç®—æ¯ä¸ªå­¦ç”Ÿæ¨¡å‹å±‚ä¹‹é—´çš„æ­¥é•¿`stride`ï¼Œé€šè¿‡`(len(layers) - 1) / (num_student_layers - 1)`è®¡ç®—å¾—åˆ°ã€‚
    stride = (len(layers) - 1) / (num_student_layers - 1)
    #åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œé€šè¿‡è®¡ç®—å¾—åˆ°çš„`stride`å’Œè¿­ä»£å˜é‡`i`ï¼Œè®¡ç®—å‡ºåº”è¯¥é€‰æ‹©çš„åŸå§‹æ¨¡å‹å±‚çš„ç´¢å¼•`idx`ã€‚ç„¶åï¼Œå°†é€‰æ‹©çš„åŸå§‹æ¨¡å‹å±‚`layers[idx]`æ·»åŠ åˆ°`student`åˆ—è¡¨ä¸­ã€‚åŒæ—¶ï¼Œä½¿ç”¨æ—¥å¿—è®°å½•å·¥å…·è®°å½•æ·»åŠ çš„å±‚çš„ç´¢å¼•ã€‚
    for i in range(num_student_layers):
        idx = round(i * stride)
        logger.info(f"Adding layer {idx} to student")
        student.append(layers[idx])
    #è¿”å›`student`åˆ—è¡¨
    return student

#åœ¨å‡½æ•°å†…éƒ¨ï¼Œé¦–å…ˆä½¿ç”¨`torch.no_grad()`è£…é¥°å™¨å°†ä¸‹é¢çš„ä»£ç åŒ…è£…èµ·æ¥ï¼Œä»¥ç¡®ä¿åœ¨å‚æ•°è£å‰ªè¿‡ç¨‹ä¸­ä¸ä¼šè¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚
@torch.no_grad()
#`ratio`æ˜¯ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œè¡¨ç¤ºè¦è£å‰ªæ‰çš„å‚æ•°æ¯”ä¾‹ã€‚
def magnitude_prune(model, ratio):
    for param in model.parameters():
        #å¯¹äºç»´åº¦ä¸º1çš„å‚æ•°ï¼Œè·³è¿‡ä¸å¤„ç†ã€‚
        if param.dim() == 1:
            continue
        #è®¡ç®—éœ€è¦è£å‰ªçš„æ•°é‡`num_prune`ï¼Œå°†å‚æ•°å…ƒç´ æ€»æ•°ä¹˜ä»¥è£å‰ªæ¯”ä¾‹å¾—åˆ°ã€‚
        num_prune = int(param.numel() * ratio)
        #ä½¿ç”¨`param.abs().view(-1).kthvalue(num_prune).values.item()`è®¡ç®—å‡ºè£å‰ªçš„é˜ˆå€¼ï¼Œè¯¥é˜ˆå€¼æ˜¯æ’åºåç¬¬`num_prune`ä¸ªæœ€å°çš„å‚æ•°ç»å¯¹å€¼ã€‚
        threshold = param.abs().view(-1).kthvalue(num_prune).values.item()
        #æ ¹æ®é˜ˆå€¼å°†å‚æ•°è¿›è¡Œè£å‰ªï¼Œé€šè¿‡å°†å‚æ•°`param`çš„ç»å¯¹å€¼ä¸é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºä¸å‚æ•°ç›¸åŒç±»å‹çš„`mask`ã€‚
        mask = (param.abs() >= threshold).to(param.dtype)
        #å°†å‚æ•°ä¸`mask`ç›¸ä¹˜ï¼Œå®ç°è£å‰ªæ“ä½œã€‚
        param.mul_(mask)

#
@torch.no_grad()
def quantize(model, bits):
    for param in model.parameters():
        if param.dim() == 1:
            continue
        min, max = param.min(), param.max()
        #è®¡ç®—å‚æ•°çš„é›¶ç‚¹`zp`ï¼Œé€šè¿‡å°†æœ€å¤§å€¼å’Œæœ€å°å€¼ç›¸åŠ é™¤ä»¥2å¾—åˆ°ã€‚
        zp = (max + min) / 2
        #è®¡ç®—å‚æ•°çš„ç¼©æ”¾å› å­`scale`ï¼Œé€šè¿‡å°†æœ€å¤§å€¼å’Œæœ€å°å€¼çš„å·®é™¤ä»¥(2 ** bits - 1)å¾—åˆ°
        scale = (max - min) / (2 ** bits - 1)
        #å¯¹å‚æ•°è¿›è¡Œé‡åŒ–æ“ä½œã€‚é¦–å…ˆï¼Œå°†å‚æ•°å‡å»é›¶ç‚¹`zp`ï¼Œç„¶åé™¤ä»¥ç¼©æ”¾å› å­`scale`ï¼Œå†ä½¿ç”¨`round_()`å‡½æ•°å››èˆäº”å…¥ã€‚æ¥ç€ï¼Œå°†å‚æ•°ä¹˜ä»¥ç¼©æ”¾å› å­`scale`ï¼Œå†åŠ ä¸Šé›¶ç‚¹`zp`ã€‚
        param.sub_(zp).div_(scale).round_().mul_(scale).add_(zp)


def parse_args():
    #åˆ›å»ºäº†ä¸€ä¸ª`argparse.ArgumentParser`å¯¹è±¡`parser`ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°çš„è§£æå·¥ä½œã€‚
    parser = argparse.ArgumentParser(
        description="Finetune a transformers model on a causal language modeling task")
    #è°ƒç”¨`parser.add_argument`æ–¹æ³•æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ã€‚
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        help="The name of the dataset to use (via the datasets library).",
    )
    parser.add_argument(
        "--dataset_config_name",
        type=str,
        default=None,
        help="The configuration name of the dataset to use (via the datasets library).",
    )
    parser.add_argument(
        "--train_file", type=str, default=None, help="A csv or a json file containing the training data."
    )
    parser.add_argument(
        "--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
    )
    parser.add_argument(
        "--validation_split_percentage",
        default=5,
        type=int,
        help="The percentage of the train set used as validation set in case there's no validation split",
    )
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        help="Path to pretrained model or model identifier from huggingface.co/models.",
        required=False,
    )
    parser.add_argument(
        "--config_name",
        type=str,
        default=None,
        help="Pretrained config name or path if not the same as model_name",
    )
    parser.add_argument(
        "--tokenizer_name",
        type=str,
        default=None,
        help="Pretrained tokenizer name or path if not the same as model_name",
    )
    parser.add_argument(
        "--use_slow_tokenizer",
        action="store_true",
        help="If passed, will use a slow tokenizer (not backed by the ğŸ¤— Tokenizers library).",
    )
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=8,
        help="Batch size (per device) for the training dataloader.",
    )
    parser.add_argument(
        "--per_device_eval_batch_size",
        type=int,
        default=8,
        help="Batch size (per device) for the evaluation dataloader.",
    )
    parser.add_argument(
        '--optimizer',
        type=str,
        default='adamw',
        help='Optimizer to use. Can be adamw or sgd',
        choices=['adamw', 'sgd']
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-5,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument("--weight_decay", type=float,
                        default=0.0, help="Weight decay to use.")
    parser.add_argument(
        "--momentum", type=float, default=0.9, help="Momentum to use for sgd optimizer."
    )
    parser.add_argument("--num_train_epochs", type=int, default=3,
                        help="Total number of training epochs to perform.")
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    parser.add_argument(
        "--lr_scheduler_type",
        type=SchedulerType,
        default="linear",
        help="The scheduler type to use.",
        choices=["linear", "cosine", "cosine_with_restarts",
                 "polynomial", "constant", "constant_with_warmup"],
    )
    parser.add_argument(
        "--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
    )
    parser.add_argument("--output_dir", type=str, default=None,
                        help="Where to store the final model.")
    parser.add_argument("--seed", type=int, default=None,
                        help="A seed for reproducible training.")
    parser.add_argument(
        "--model_type",
        type=str,
        default=None,
        help="Model type to use if training from scratch.",
        choices=MODEL_TYPES,
    )
    parser.add_argument(
        "--block_size",
        type=int,
        default=None,
        help=(
            "Optional input sequence length after tokenization. The training dataset will be truncated in block of"
            " this size for training. Default to the model max input length for single sentence inputs (take into"
            " account special tokens)."
        ),
    )
    parser.add_argument(
        "--preprocessing_num_workers",
        type=int,
        default=88,
        help="The number of processes to use for the preprocessing.",
    )
    parser.add_argument(
        "--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
    )
    parser.add_argument(
        "--no_keep_linebreaks", action="store_true", help="Do not keep line breaks when using TXT files."
    )
    parser.add_argument(
        "--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
    )
    parser.add_argument("--hub_token", type=str,
                        help="The token to use to push to the Model Hub.")
    parser.add_argument(
        "--checkpointing_steps",
        type=str,
        default=None,
        help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
    )
    parser.add_argument(
        "--report_to",
        type=str,
        default=None,
        help=(
            'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
            ' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations.'
            "Only applicable when `--with_tracking` is passed."
        ),
    )
    parser.add_argument(
        '--no_save_model',
        action='store_true',
        help='Whether or not to save the model.'
    )
    parser.add_argument(
        '--kd_weight',
        type=float,
        default=0.0,
        help='Weight of the knowledge distillation loss.'
    )
    parser.add_argument(
        '--lm_weight',
        type=float,
        default=1.0,
        help='Weight of the knowledge distillation loss.'
    )
    parser.add_argument(
        '--train_tokenized_dataset',
        type=str,
        default=None,
        help='Path to the tokenized training dataset.'
    )
    parser.add_argument(
        '--val_tokenized_dataset',
        type=str,
        default=None,
        help='Path to the tokenized validation dataset.'
    )
    parser.add_argument(
        "--train_num_samples",
        type=int,
        default=None,
        help="The number of samples to use for training set.",
    )
    parser.add_argument(
        "--validation_num_samples",
        type=int,
        default=None,
        help="The number of samples to use for validation set.",
    )
    parser.add_argument(
        '--eval_steps',
        type=int,
        default=200,
    )

    parser.add_argument(
        '--num_student_layers',
        type=int,
        default=None,
        help='Number of layers in the student model.'
    )

    parser.add_argument(
        '--load_student',
        type=str,
        default=None,
        help='Path to the student model'
    )

    parser.add_argument(
        '--student_l_pad',
        type=int,
        default=0,
    )

    parser.add_argument(
        '--student_r_pad',
        type=int,
        default=0,
    )

    parser.add_argument(
        '--student_layer_selection_strategy',
        type=str,
        default='uniform',
        help='Layer selection strategy',
        choices=['uniform', 'random', 'changes']
    )

    parser.add_argument(
        '--restart_training',
        action='store_true',
        help='Whether to restart training of all dataset.'
    )

    parser.add_argument(
        '--train_module',
        type=str,
        default='student',
        help='Part of the model to train.',
        choices=['student', 'adapter', 'all']
    )
    parser.add_argument(
        '--max_grad_norm',
        type=float,
        default=1.0,
        help='Max gradient norm.'
    )

    parser.add_argument(
        '--magnitude_pruning_ratio',
        type=float,
        default=0.0,
        help='Magnitude pruning ratio.'
    )

    parser.add_argument(
        '--weight_quantization_bits',
        type=int,
        default=None,
        help='Weight quantization bits.'
    )
    parser.add_argument(
        "--mlm_probability", type=float, default=0.15, help="Ratio of tokens to mask for masked language modeling loss"
    )

    # vit
    parser.add_argument("--train_dir", type=str, default=None,
                        help="A folder containing the training data.")
    parser.add_argument("--validation_dir", type=str, default=None,
                        help="A folder containing the validation data.")
    parser.add_argument(
        "--max_train_samples",
        type=int,
        default=None,
        help=(
            "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        ),
    )
    parser.add_argument(
        "--max_eval_samples",
        type=int,
        default=None,
        help=(
            "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
            "value if set."
        ),
    )
    parser.add_argument(
        "--train_val_split",
        type=float,
        default=0.15,
        help="Percent to split off of train for validation",
    )
    parser.add_argument(
        "--ignore_mismatched_sizes",
        action="store_true",
        help="Whether or not to enable to load a pretrained model whose head dimensions are different.",
    )

    parser.add_argument(
        "--max_length",
        type=int,
        default=128,
        help=(
            "The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
            " sequences shorter will be padded if `--pad_to_max_lengh` is passed."
        ),
    )
    parser.add_argument(
        "--pad_to_max_length",
        action="store_true",
    )

    parser.add_argument(
        '--freeze_bottom',
        action='store_true',
    )
    
    parser.add_argument(
        '--no_teacher',
        action='store_true',
    )

    parser.add_argument(
        '--classifier_lr_multiplier',
        type=float,
        default=1.0,
    )
    
    parser.add_argument(
        '--select_by_kd',
        action='store_true',
    )
    
    parser.add_argument(
        '--use_pt_imagefolder',
        action='store_true',
    )
    
    parser.add_argument(
        '--num_workers',
        type=int,
        default=12,
    )

    parser.add_argument(
        '--train_lm_head',
        action='store_true',
    )
    parser.add_argument(
        '--save_module',
        type=str,
        default='student',
        choices=['student', 'adapter', 'all']
    )
    
    parser.add_argument(
        '--load_adapter',
        type=str,
        default=None,
        help='Path to the student model'
    )

    parser.add_argument(
        '--tasks',
        type=str,
        default='piqa',
        help='Evaluation tasks',
    )
    
    parser.add_argument(
        '--use_adapter',
        action='store_true',
    )
    
    parser.add_argument(
        '--use_lora',
        action='store_true',
    )
    
    parser.add_argument(
        '--use_bitfit',
        action='store_true',
    )
    
    parser.add_argument(
        '--lora_rank',
        type=int,
        default=4,
        help='Rank of the LoRA matrix',
    )

    parser.add_argument(
        '--lora_alpha',
        type=float,
        default=32,
        help='Alpha of the LoRA matrix',
    )

    parser.add_argument(
        '--adapter_size',
        type=int,
        default=64,
        help='Size of the adapter',
    )
    #
    parser.add_argument
    #`parser.parse_args()`æ–¹æ³•ä¼šè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªå‘½åç©ºé—´å¯¹è±¡`args`ï¼Œå…¶ä¸­åŒ…å«äº†å‘½ä»¤è¡Œå‚æ•°çš„ä¿¡æ¯ã€‚é€šè¿‡ç‚¹æ“ä½œç¬¦å¯ä»¥è®¿é—®è¿™äº›å‚æ•°çš„å€¼ã€‚
    args = parser.parse_args()

    return args
#ä½¿ç”¨å¤šä¸ª`if-elif-else`è¯­å¥æ¥ç¡®å®šä¼ å…¥çš„`model`å±äºå“ªç§ç±»å‹ï¼Œå¹¶ç›¸åº”åœ°è·å–å¯¹åº”çš„å±‚æˆ–æ¨¡å—ã€‚
def get_layers(model):
    if isinstance(model, OPTForCausalLM):
        layers = model.model.decoder.layers
    elif isinstance(model, GPT2LMHeadModel):
        layers = model.transformer.h
    elif isinstance(model, BloomForCausalLM):
        layers = model.transformer.h
    elif isinstance(model, ViTForImageClassification):
        layers = model.vit.encoder.layer
    elif isinstance(model, CLIPViTForImageClassification):
        layers = model.vit.encoder.layers
    elif isinstance(model, EVAViTForImageClassification):
        layers = model.blocks
    else:
        raise NotImplementedError
    return layers

def set_layers(model, layers):
    if isinstance(model, OPTForCausalLM):
        model.model.decoder.layers = layers
    elif isinstance(model, GPT2LMHeadModel):
        model.transformer.h = layers
    elif isinstance(model, BloomForCausalLM):
        model.transformer.h = layers
    elif isinstance(model, ViTForImageClassification):
        model.vit.encoder.layer = layers
    elif isinstance(model, CLIPViTForImageClassification):
        model.vit.encoder.layers = layers
    elif isinstance(model, EVAViTForImageClassification):
        model.blocks = layers
    else:
        raise NotImplementedError

#è®¾ç½®ä¸€ä¸ªç”¨äºè®­ç»ƒçš„å¸ˆç”Ÿæ¨¡å‹
def setup_teacher_student(model, args, accelerator):
    #åœ¨å‡½æ•°å†…éƒ¨ï¼Œé¦–å…ˆé€šè¿‡å¾ªç¯å°†`model`ä¸­çš„æ‰€æœ‰å‚æ•°çš„`requires_grad`å±æ€§è®¾ä¸º`False`ï¼Œå³å†»ç»“æ‰€æœ‰å‚æ•°ï¼Œä½¿å…¶ä¸å¯è®­ç»ƒã€‚
    for param in model.parameters():
        param.requires_grad = False
    #è°ƒç”¨`get_layers`å‡½æ•°è·å–`model`çš„å±‚æˆ–æ¨¡å—ï¼Œå¹¶å°†ç»“æœä¿å­˜åœ¨`layers`å˜é‡ä¸­ã€‚
    layers = get_layers(model)
    #æ ¹æ®`args.student_l_pad`å’Œ`args.student_r_pad`ç¡®å®šéœ€è¦é€‰æ‹©çš„å±‚çš„èŒƒå›´ï¼Œ
    l, r = args.student_l_pad, len(layers) - args.student_r_pad
    #ç„¶åæ ¹æ®`args.load_student`æ˜¯å¦æœ‰å€¼é€‰æ‹©åŠ è½½å·²æœ‰çš„å­¦ç”Ÿæ¨¡å‹æˆ–è€…åˆ›å»ºæ–°çš„å­¦ç”Ÿæ¨¡å‹ã€‚
    #è‹¥åŠ è½½å·²æœ‰çš„å­¦ç”Ÿæ¨¡å‹ï¼Œåˆ™ä»`student_state_dict`åŠ è½½å‚æ•°ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™`student`ã€‚
    
    #å¦‚æœæä¾›äº† `args.load_student`ï¼Œåˆ™ä»æŒ‡å®šä½ç½®åŠ è½½ä¸€ä¸ªå·²å­˜åœ¨çš„å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™ `student` å˜é‡ã€‚
    #å¦åˆ™ï¼Œé€šè¿‡æ·±æ‹·è´æŒ‡å®šèŒƒå›´å†…çš„å±‚æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„å­¦ç”Ÿæ¨¡å‹ã€‚
    if args.load_student:
        student_state_dict = torch.load(os.path.join(
            args.load_student, 'student.pt'), map_location='cpu')
        student_layers_len = len(
            set([k.split('.')[0] for k in student_state_dict.keys()]))
        logger.info(
            f"Loading student module from {args.load_student} with {student_layers_len} layers.")
        student = deepcopy(layers[:student_layers_len])
        student.load_state_dict(student_state_dict)
    else:
        student = deepcopy(layers[l:r])
    #å¦‚æœ `args.student_layer_selection_strategy` è®¾ç½®ä¸º `'uniform'`ï¼Œ
    #åˆ™ä»å­¦ç”Ÿæ¨¡å‹ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†å±‚ï¼Œå…¶åˆ†å¸ƒå‡åŒ€ä¸”å¤§å°ç”± `args.num_student_layers` æŒ‡å®šã€‚
    if args.student_layer_selection_strategy == 'uniform':
        student = uniform_choose_layers(student, args.num_student_layers)
    else:
        raise NotImplementedError
    #å°†`student`ç§»åŠ¨åˆ°åŠ é€Ÿå™¨è®¾å¤‡ä¸Šã€‚
    student = student.to(accelerator.device)
    #å¦‚æœ`args.magnitude_pruning_ratio`å¤§äº0ï¼Œåˆ™è°ƒç”¨`magnitude_prune`å‡½æ•°å¯¹`student`è¿›è¡Œå‰ªææ“ä½œã€‚
    if args.magnitude_pruning_ratio > 0:
        logger.info(
            f"Pruning student module with magnitude ratio {args.magnitude_pruning_ratio}")
        magnitude_prune(student, args.magnitude_pruning_ratio)
    #å¦‚æœ`args.weight_quantization_bits`ä¸ä¸º`None`ï¼Œåˆ™è°ƒç”¨`quantize`å‡½æ•°å¯¹`student`è¿›è¡Œæƒé‡é‡åŒ–æ“ä½œã€‚
    if args.weight_quantization_bits is not None:
        logger.info(
            f"Quantizing student module with {args.weight_quantization_bits} bits")
        quantize(student, args.weight_quantization_bits)

    #å¦‚æœ`args.train_module`ç­‰äº`student`ï¼Œåˆ™å°†`student`ä¸­çš„å‚æ•°è®¾ç½®ä¸ºå¯è®­ç»ƒã€‚
    if args.train_module == 'student':
        for param in student.parameters():
            param.data = param.data.float()
            param.requires_grad = True
    #å¦‚æœ`args.train_module`ç­‰äº`adapter`ï¼Œåˆ™å°†`student`ä¸­çš„å‚æ•°å†»ç»“ï¼Œå¹¶æ ¹æ®`args.freeze_bottom`çš„å€¼é€‰æ‹©æ˜¯å¦å†»ç»“`layers`çš„åº•éƒ¨å±‚ã€‚
    elif args.train_module == 'adapter':
        for param in student.parameters():
            param.requires_grad = False
        if not args.freeze_bottom:
            for param in layers[:l].parameters():
                param.data = param.data.float()
                param.requires_grad = True
        for param in layers[r:].parameters():
            param.data = param.data.float()
            param.requires_grad = True
    #å¦‚æœ`args.train_module`ç­‰äº`all`ï¼Œåˆ™å°†`student`å’Œ`layers`çš„æ‰€æœ‰å‚æ•°è®¾ç½®ä¸ºå¯è®­ç»ƒã€‚
    elif args.train_module == 'all':
        for param in student.parameters():
            param.data = param.data.float()
            param.requires_grad = True
        for param in layers[:l].parameters():
            param.data = param.data.float()
            param.requires_grad = True
        for param in layers[r:].parameters():
            param.data = param.data.float()
            param.requires_grad = True
    else:
        raise NotImplementedError

    model.student = student

    model.teacher = layers[l:r].half()
    
    model.adapter = layers[:l] + layers[r:]

    for param in model.teacher.parameters():
        param.requires_grad = False

    add_prologue(model.student[0], None)
    add_epilogue(model.student[-1], None)
    #å°†å­¦ç”Ÿæ¨¡å‹çš„ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚åˆ†åˆ«èµ‹å€¼ç»™ `model.student_l` å’Œ `model.student_r`ã€‚
    model.student_l = model.student[0]
    model.student_r = model.student[-1]
    #ç¡®å®šå­¦ç”Ÿæ¨¡å‹å±‚æ•°ï¼Œå¹¶è®°å½•æ—¥å¿—ã€‚
    num_student_layers = len(model.student)
    logger.info(f"Number of student layers: {num_student_layers}")
    #æ ¹æ® `args.train_module` çš„å€¼ï¼Œå°†é€‚å½“çš„å¯è®­ç»ƒæ¨¡å—èµ‹å€¼ç»™ `model.trainable_module`ã€‚
    if args.train_module == 'student':
        model.trainable_module = model.student
    elif args.train_module == 'adapter':
        model.trainable_module = model.adapter
    elif args.train_module == 'all':
        model.trainable_module = model.student + model.adapter
    else:
        raise NotImplementedError
    #è¿›è¡Œåƒåœ¾å›æ”¶å¹¶æ¸…é™¤ GPU ç¼“å­˜ã€‚
    gc.collect()
    torch.cuda.empty_cache()
    return model

#è¯¥å‡½æ•°æ¥å—ä¸€ä¸ª `model` å’Œ `args` ä½œä¸ºè¾“å…¥ã€‚å®ƒç”¨äºå°†æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ›¿æ¢ä¸ºæ•™å¸ˆæ¨¡å‹ã€‚
#è¾“å…¥çš„ `model` å¯ä»¥æ˜¯ä¸åŒç±»å‹çš„æ¨¡å‹ã€‚è¦æ›¿æ¢çš„å…·ä½“å±‚ç”± `args.student_l_pad` å’Œ `args.student_r_pad` çš„å€¼ç¡®å®šã€‚
#åœ¨æ›¿æ¢å±‚ä¹‹åï¼Œå‡½æ•°ä¼šç›¸åº”åœ°æ›´æ–° `model`ã€‚
def to_teacher(model, args):
    l = args.student_l_pad
    if isinstance(model, OPTForCausalLM):
        r = len(model.model.decoder.layers) - args.student_r_pad
        model.model.decoder.layers = model.model.decoder.layers[
            :l] + model.teacher + model.model.decoder.layers[r:]
    elif isinstance(model, GPT2LMHeadModel):
        r = len(model.transformer.h) - args.student_r_pad
        model.transformer.h = model.transformer.h[:l] + \
            model.teacher + model.transformer.h[r:]
    elif isinstance(model, BloomForCausalLM):
        r = len(model.transformer.h) - args.student_r_pad
        model.transformer.h = model.transformer.h[:l] + \
            model.teacher + model.transformer.h[r:]
    elif isinstance(model, ViTForImageClassification):
        r = len(model.vit.encoder.layer) - args.student_r_pad
        model.vit.encoder.layer = model.vit.encoder.layer[:l] + \
            model.teacher + model.vit.encoder.layer[r:]
    elif isinstance(model, CLIPViTForImageClassification):
        r = len(model.vit.encoder.layers) - args.student_r_pad
        model.vit.encoder.layers = model.vit.encoder.layers[:l] + \
            model.teacher + model.vit.encoder.layers[r:]
    elif isinstance(model, EVAViTForImageClassification):
        r = len(model.blocks) - args.student_r_pad
        model.blocks = model.blocks[:l] + \
            model.teacher + model.blocks[r:]
    else:
        raise NotImplementedError

#è¯¥å‡½æ•°ä¸ `to_teacher` ç±»ä¼¼ï¼Œä½†å®ƒå°†æŒ‡å®šçš„å±‚æ›¿æ¢ä¸ºå­¦ç”Ÿæ¨¡å‹è€Œä¸æ˜¯æ•™å¸ˆæ¨¡å‹ã€‚
def to_student(model, args):
    l = args.student_l_pad
    if isinstance(model, OPTForCausalLM):
        r = len(model.model.decoder.layers) - args.student_r_pad
        model.model.decoder.layers = model.model.decoder.layers[
            :l] + model.student + model.model.decoder.layers[r:]
    elif isinstance(model, GPT2LMHeadModel):
        r = len(model.transformer.h) - args.student_r_pad
        model.transformer.h = model.transformer.h[:l] + \
            model.student + model.transformer.h[r:]
    elif isinstance(model, BloomForCausalLM):
        r = len(model.transformer.h) - args.student_r_pad
        model.transformer.h = model.transformer.h[:l] + \
            model.student + model.transformer.h[r:]
    elif isinstance(model, ViTForImageClassification):
        r = len(model.vit.encoder.layer) - args.student_r_pad
        model.vit.encoder.layer = model.vit.encoder.layer[:l] + \
            model.student + model.vit.encoder.layer[r:]
    elif isinstance(model, CLIPViTForImageClassification):
        r = len(model.vit.encoder.layers) - args.student_r_pad
        model.vit.encoder.layers = model.vit.encoder.layers[:l] + \
            model.student + model.vit.encoder.layers[r:]
    elif isinstance(model, EVAViTForImageClassification):
        r = len(model.blocks) - args.student_r_pad
        model.blocks = model.blocks[:l] + \
            model.student + model.blocks[r:]
    else:
        raise NotImplementedError

#è¯¥å‡½æ•°è®¡ç®—æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†è’¸é¦æŸå¤±ã€‚
def get_kd_loss(model):
    kwargs = model.student_l.input_kwargs
    args = model.student_l.input_args
    output_teacher = args[0].to(torch.float16)
    args = list(args[1:])
    for i, arg in enumerate(args):
        if torch.is_tensor(arg) and arg.dtype == torch.float32:
            args[i] = arg.to(torch.float16)
    args = tuple(args)

    for k, v in kwargs.items():
        if torch.is_tensor(v) and v.dtype == torch.float32:
            kwargs[k] = v.to(torch.float16)

    with torch.no_grad():
        model.teacher.eval()
        for teacher_layer in model.teacher:
            output_teacher = teacher_layer(output_teacher, *args, **kwargs)
            if isinstance(output_teacher, tuple):
                output_teacher = output_teacher[0]

    output_student = model.student_r.cached_output.float()
    output_teacher = output_teacher.float()

    std = output_teacher.pow(2).mean().sqrt()
    kd_loss = (output_teacher - output_student).div(std).pow(2).mean()
    return kd_loss

#ç”¨äºè®¾ç½®å¯è®­ç»ƒçš„åˆ†ç±»å¤´ã€‚æ ¹æ®æ¨¡å‹çš„ç±»å‹ï¼Œè¯¥å‡½æ•°ä¼šå°†åˆ†ç±»å™¨å±‚çš„å‚æ•°è®¾ç½®ä¸ºå¯è®­ç»ƒï¼Œå¹¶å°†å‚æ•°çš„æ•°æ®ç±»å‹è½¬æ¢ä¸º floatã€‚
def setup_trainable_classification_head(model):
    # Setup trainable classification heads
    if isinstance(model, ViTForImageClassification):
        for param in model.classifier.parameters():
            param.requires_grad = True
            param.data = param.data.float()
    elif isinstance(model, CLIPViTForImageClassification):
        for param in model.classifier.parameters():
            param.requires_grad = True
            param.data = param.data.float()
    elif isinstance(model, EVAViTForImageClassification):
        for param in model.classifier.parameters():
            param.requires_grad = True
            param.data = param.data.float()
    else:
        raise NotImplementedError
#è¯¥å‡½æ•°ç”¨äºåŠ è½½é€‚é…å™¨ã€‚æ ¹æ®æ¨¡å‹çš„ç±»å‹ï¼ŒåŠ è½½é€‚é…å™¨å±‚çš„çŠ¶æ€å­—å…¸ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™å¯¹åº”çš„æ¨¡å‹å±‚ã€‚åŠ è½½é€‚é…å™¨åï¼Œå‡½æ•°è¿”å›æ›´æ–°åçš„æ¨¡å‹ã€‚
def load_adapter(model, adapter_state_dict, args):
    l = args.student_l_pad
    if isinstance(model, OPTForCausalLM):
        r = len(model.model.decoder.layers) - args.student_r_pad
        adapter_layers = model.model.decoder.layers[:l] + model.model.decoder.layers[r:]
        adapter_layers.load_state_dict(adapter_state_dict)
    elif isinstance(model, GPT2LMHeadModel):
        r = len(model.transformer.h) - args.student_r_pad
        adapter_layers = model.transformer.h[:l] + model.transformer.h[r:]
        adapter_layers.load_state_dict(adapter_state_dict)
    elif isinstance(model, BloomForCausalLM):
        r = len(model.transformer.h) - args.student_r_pad
        adapter_layers = model.transformer.h[:l] + model.transformer.h[r:]
        adapter_layers.load_state_dict(adapter_state_dict)
    else:
        raise NotImplementedError
    return model
#è¯¥å‡½æ•°ç”¨äºåŠ è½½å­¦ç”Ÿæ¨¡å‹ã€‚
def load_student(model, student_state_dict, args):
    #æ ¹æ® `args.student_l_pad` è·å–å­¦ç”Ÿæ¨¡å‹ä¸­éœ€è¦åŠ è½½çš„å±‚æ•°ã€‚
    l = args.student_l_pad
    
    student_layers_len = len(
        set([k.split('.')[0] for k in student_state_dict.keys()]))
    logger.info(f"Loading student module from with {student_layers_len} layers.")
    #åŠ è½½ç›¸åº”å±‚æ•°çš„å­¦ç”Ÿæ¨¡å‹å‚æ•°ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™å¯¹åº”çš„æ¨¡å‹å±‚ã€‚åŠ è½½å­¦ç”Ÿæ¨¡å‹åï¼Œå‡½æ•°è¿”å›æ›´æ–°åçš„æ¨¡å‹ã€‚
    if isinstance(model, OPTForCausalLM):
        r = len(model.model.decoder.layers) - args.student_r_pad
        student_layers = model.model.decoder.layers[l:l+student_layers_len]
        student_layers.load_state_dict(student_state_dict)
        model.model.decoder.layers = model.model.decoder.layers[:l] + \
            student_layers + model.model.decoder.layers[r:]
    elif isinstance(model, GPT2LMHeadModel):
        r = len(model.transformer.h) - args.student_r_pad
        student_layers = model.transformer.h[l:l+student_layers_len]
        student_layers.load_state_dict(student_state_dict)
        model.transformer.h = model.transformer.h[:l] + \
            student_layers + model.transformer.h[r:]
    elif isinstance(model, BloomForCausalLM):
        r = len(model.transformer.h) - args.student_r_pad
        student_layers = model.transformer.h[l:l+student_layers_len]
        student_layers.load_state_dict(student_state_dict)
        model.transformer.h = model.transformer.h[:l] + \
            student_layers + model.transformer.h[r:]
    else:
        raise NotImplementedError
    return model
#ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•å’Œæ–‡ä»¶åã€‚å‡½æ•°é¦–å…ˆå°†çŠ¶æ€å­—å…¸ä¸­çš„å‚æ•°è½¬æ¢ä¸º float16 æ•°æ®ç±»å‹ï¼Œå¹¶ç§»åŠ¨åˆ° CPU ä¸Šã€‚
#ç„¶åï¼Œä½¿ç”¨ `torch.save` æ–¹æ³•å°†çŠ¶æ€å­—å…¸ä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•å’Œæ–‡ä»¶åä¸­ã€‚
def save_state_dict(state_dict, output_dir, filename):
    for k in state_dict:
        state_dict[k] = state_dict[k].to(torch.float16).cpu()
    torch.save(state_dict, os.path.join(output_dir, filename))