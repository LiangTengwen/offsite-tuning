# coding=utf-8
# Copyright 2022 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Finetuning any ğŸ¤— Transformers model for image classification leveraging ğŸ¤— Accelerate."""
import argparse
import json
import logging
import math
import os
import sys

import datasets
import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
#è¿™æ®µä»£ç å¯¼å…¥äº†ä¸€äº›æ¥è‡ªtorchvision.transformsæ¨¡å—çš„å›¾åƒè½¬æ¢å‡½æ•°ã€‚
#è¿™äº›å‡½æ•°å¯ç”¨äºå¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†å’Œæ•°æ®å¢å¼ºï¼Œä»¥å‡†å¤‡è¾“å…¥ç»™æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

#ä¸‹é¢æ˜¯å¯¼å…¥çš„å‡½æ•°åŠå…¶åŠŸèƒ½çš„ç®€è¦è¯´æ˜ï¼š

#`CenterCrop`: å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªï¼Œé€šè¿‡æŒ‡å®šè£å‰ªçš„è¾“å‡ºå°ºå¯¸ï¼Œå°†å›¾åƒè£å‰ªä¸ºä¸­å¿ƒåŒºåŸŸçš„æ­£æ–¹å½¢æˆ–çŸ©å½¢ã€‚
# `Compose`: å°†å¤šä¸ªå›¾åƒè½¬æ¢å‡½æ•°ç»„åˆæˆä¸€ä¸ªåºåˆ—ï¼Œä¾æ¬¡åº”ç”¨è¿™äº›å‡½æ•°ã€‚
# `Normalize`: å¯¹å›¾åƒè¿›è¡Œæ ‡å‡†åŒ–ï¼Œé€šè¿‡å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®æ¥å°†å›¾åƒçš„åƒç´ å€¼è°ƒæ•´åˆ°ç‰¹å®šèŒƒå›´ã€‚
# `RandomHorizontalFlip`: éšæœºæ°´å¹³ç¿»è½¬å›¾åƒï¼Œä»¥å¢åŠ è®­ç»ƒæ•°æ®çš„å˜åŒ–æ€§å’Œå¤šæ ·æ€§ã€‚
# `RandomResizedCrop`: éšæœºè£å‰ªå›¾åƒï¼Œå¹¶å°†å…¶ç¼©æ”¾åˆ°æŒ‡å®šçš„å¤§å°ã€‚è¿™ä¸ªå‡½æ•°å¯ä»¥åœ¨è£å‰ªå’Œç¼©æ”¾çš„è¿‡ç¨‹ä¸­å¼•å…¥éšæœºæ€§ï¼Œä»¥å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚
# `Resize`: è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œé€šè¿‡æŒ‡å®šè¾“å‡ºå°ºå¯¸æˆ–ç¼©æ”¾å› å­æ¥æ”¹å˜å›¾åƒçš„å¤§å°ã€‚
# `ToTensor`: å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ï¼ˆtensorï¼‰ï¼Œæ–¹ä¾¿åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚
# è¿™äº›è½¬æ¢å‡½æ•°å¯ç”¨äºåˆ›å»ºå›¾åƒæ•°æ®é›†çš„é¢„å¤„ç†ç®¡é“ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†å’Œæ•°æ®å¢å¼ºï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚
from torchvision.transforms import (
    CenterCrop,
    Compose,
    Normalize,
    RandomHorizontalFlip,
    RandomResizedCrop,
    Resize,
    ToTensor,
)
from tqdm.auto import tqdm

import evaluate
import transformers
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed

#è¿™æ®µä»£ç å¯¼å…¥äº†ä¸€äº›æ¥è‡ª`transformers`åº“çš„æ¨¡å—å’Œç±»ï¼Œè¿™äº›æ¨¡å—å’Œç±»æä¾›äº†ç”¨äºå›¾åƒåˆ†ç±»å’Œè§†è§‰æ¨¡å‹çš„ä¸€äº›åŠŸèƒ½å’Œå·¥å…·ã€‚

# ä¸‹é¢æ˜¯å¯¼å…¥çš„ä¸€äº›æ¨¡å—å’Œç±»çš„ç®€è¦è¯´æ˜ï¼š

# `AutoConfig`: ç”¨äºè‡ªåŠ¨é…ç½®å„ç§æ¨¡å‹çš„é…ç½®ç±»ã€‚å¯ä»¥æ ¹æ®å…·ä½“çš„æ¨¡å‹ç±»åç§°æˆ–é¢„è®­ç»ƒæ¨¡å‹çš„åç§°ï¼Œè‡ªåŠ¨åŠ è½½ç›¸åº”çš„æ¨¡å‹é…ç½®ã€‚
# `AutoFeatureExtractor`: ç”¨äºè‡ªåŠ¨åŠ è½½å’Œé…ç½®å„ç§ç‰¹å¾æå–å™¨çš„ç±»ã€‚å¯ä»¥æ ¹æ®å…·ä½“çš„æ¨¡å‹ç±»åç§°æˆ–é¢„è®­ç»ƒæ¨¡å‹çš„åç§°ï¼Œè‡ªåŠ¨åŠ è½½ç›¸åº”çš„ç‰¹å¾æå–å™¨ã€‚
# `AutoModelForImageClassification`: ç”¨äºè‡ªåŠ¨åŠ è½½å’Œé…ç½®å„ç§å›¾åƒåˆ†ç±»æ¨¡å‹çš„ç±»ã€‚å¯ä»¥æ ¹æ®å…·ä½“çš„æ¨¡å‹ç±»åç§°æˆ–é¢„è®­ç»ƒæ¨¡å‹çš„åç§°ï¼Œè‡ªåŠ¨åŠ è½½ç›¸åº”çš„å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚
# `get_scheduler`: ç”¨äºè·å–ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨çš„å‡½æ•°ã€‚å¯ä»¥æ ¹æ®æŒ‡å®šçš„å‚æ•°æ¥è·å–ä¸åŒç±»å‹çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
# `CLIPVisionConfig`: ç”¨äºé…ç½®CLIPè§†è§‰æ¨¡å‹çš„é…ç½®ç±»ã€‚CLIPï¼ˆContrastive Language-Image Pretrainingï¼‰æ˜¯ä¸€ç§è”åˆè®­ç»ƒè¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚
# `CLIPVisionModel`: ç”¨äºåŠ è½½å’Œä½¿ç”¨CLIPè§†è§‰æ¨¡å‹çš„ç±»ã€‚
from transformers import (
    AutoConfig,
    AutoFeatureExtractor,
    AutoModelForImageClassification,
    get_scheduler,
    CLIPVisionConfig,
    CLIPVisionModel,
)


from offsite_tuning.utils import (
    parse_args,
    setup_teacher_student,
    get_kd_loss,
    to_teacher,
    to_student,
    setup_trainable_classification_head
)

from offsite_tuning.models.clip_vit import CLIPViTForImageClassification
from offsite_tuning.models.eva_vit import EVAViTForImageClassification
import gc

logger = get_logger(__name__)


def main():
    args = parse_args()

    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers
    # in the environment
    #å®šä¹‰äº†ä¸€ä¸ªç©ºçš„å­—å…¸å¯¹è±¡`accelerator_log_kwargs = {}`ï¼Œç”¨äºä¿å­˜åŠ é€Ÿå™¨æ—¥å¿—ç›¸å…³çš„å‚æ•°ã€‚
    accelerator_log_kwargs = {}
    #å°†`args.report_to`çš„å€¼èµ‹ç»™`log_with`é”®ï¼Œå°†`args.output_dir`çš„å€¼èµ‹ç»™`logging_dir`é”®ï¼Œ
    # å°†è¿™ä¸¤ä¸ªé”®å€¼å¯¹æ·»åŠ åˆ°`accelerator_log_kwargs`å­—å…¸ä¸­ã€‚
    # è¿™æ ·åšçš„ç›®çš„æ˜¯å°†æ—¥å¿—è¾“å‡ºæ–¹å¼ï¼ˆlog_withï¼‰å’Œæ—¥å¿—ä¿å­˜çš„ç›®å½•ï¼ˆlogging_dirï¼‰ä¼ é€’ç»™åŠ é€Ÿå™¨å¯¹è±¡è¿›è¡Œé…ç½®ã€‚
    accelerator_log_kwargs["log_with"] = args.report_to
    accelerator_log_kwargs["logging_dir"] = args.output_dir

    #å®ä¾‹åŒ–`Accelerator`ç±»ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸º`accelerator`çš„åŠ é€Ÿå™¨å¯¹è±¡ï¼Œ
    # å¹¶ä¼ é€’äº†æ¢¯åº¦ç´¯ç§¯æ­¥æ•°å‚æ•°ï¼ˆgradient_accumulation_stepsï¼‰å’Œä¹‹å‰åˆ›å»ºçš„`accelerator_log_kwargs`å­—å…¸ä½œä¸ºå…³é”®å­—å‚æ•°ã€‚
    #åŠ é€Ÿå™¨å¯¹è±¡å¯ä»¥ç”¨äºç®¡ç†å’ŒåŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—ï¼ŒåŒæ—¶å¯ä»¥æä¾›æ—¥å¿—è®°å½•å’Œç®¡ç†çš„åŠŸèƒ½ã€‚
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

    # Handle the repository creation
    #é€šè¿‡`accelerator.is_main_process`å’Œ`args.output_dir`ä¸ä¸ºç©ºè¿›è¡Œåˆ¤æ–­ï¼Œ
    # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ä¸”`output_dir`å‚æ•°ä¸ä¸ºç©ºï¼Œåˆ™è°ƒç”¨`os.makedirs()`å‡½æ•°åˆ›å»ºè¾“å‡ºç›®å½•`args.output_dir`ã€‚
    # `os.makedirs()`å‡½æ•°ä¼šé€’å½’åˆ›å»ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼ˆé€šè¿‡`exist_ok=True`å‚æ•°è®¾ç½®ï¼‰ã€‚
    if accelerator.is_main_process and args.output_dir is not None:
        os.makedirs(args.output_dir, exist_ok=True)

    #æ‰“å°åŠ é€Ÿå™¨çŠ¶æ€ä¿¡æ¯
    logger.info(accelerator.state)
    # Make one log on every process with the configuration for debugging.
    #æ—¥å¿—è¾“å‡ºçš„æ ¼å¼è®¾ç½®ä¸º"% (asctime)s -% (levelname)s -% (name)s -% (message)s"ï¼Œæ—¥æœŸæ ¼å¼ä¸º"%m/%d/%Y %H:%M:%S"ï¼Œ
    # æ—¥å¿—çº§åˆ«ä¸º`logging.INFO`ï¼Œæ—¥å¿—å¤„ç†ç¨‹åºè®¾ç½®ä¸ºå°†æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºæµï¼ˆ`StreamHandler(sys.stdout)`ï¼‰å’Œæ—¥å¿—æ–‡ä»¶ï¼ˆâ€œlog.txtâ€ï¼Œä½äº`args.output_dir`ç›®å½•ä¸‹ï¼‰ã€‚
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(os.path.join(args.output_dir, "log.txt"))
        ] if accelerator.is_main_process else []
    )
    logger.info(accelerator.state, main_process_only=False)
    
    #æ ¹æ®å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œé€šè¿‡æ¡ä»¶è¡¨è¾¾å¼æ¥è®¾ç½®æ—¥å¿—è®°å½•çš„å¤„ç†ç¨‹åºã€‚
    # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ï¼Œä½¿ç”¨ä¹‹å‰è®¾ç½®çš„æ—¥å¿—å¤„ç†ç¨‹åºï¼›å¦åˆ™ï¼Œè®¾ç½®ä¸€ä¸ªç©ºçš„åˆ—è¡¨ï¼Œå³ä¸æ‰§è¡Œæ—¥å¿—è®°å½•ã€‚
    
    #è®¾ç½®æ•°æ®é›†çš„æ—¥å¿—è®°å½•çº§åˆ«ã€‚å¦‚æœå½“å‰è¿›ç¨‹æ˜¯æœ¬åœ°ä¸»è¿›ç¨‹ï¼ˆå³éåˆ†å¸ƒå¼è®­ç»ƒï¼‰ï¼Œåˆ™å°†æ•°æ®é›†å’Œtransformersæ¨¡å—çš„æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º`INFO`ï¼›å¦åˆ™ï¼Œå°†å®ƒä»¬çš„æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º`ERROR`ï¼Œå³ä»…è®°å½•é”™è¯¯ä¿¡æ¯ã€‚
    if accelerator.is_local_main_process:
        datasets.utils.logging.set_verbosity_warning()
        transformers.utils.logging.set_verbosity_info()
    else:
        datasets.utils.logging.set_verbosity_error()
        transformers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    if args.seed is not None:
        set_seed(args.seed)

    #è¿›å…¥åŠ é€Ÿå™¨çš„ç­‰å¾…çŠ¶æ€ï¼Œä»¥ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½è¾¾åˆ°åŒæ­¥çŠ¶æ€ã€‚
    accelerator.wait_for_everyone()

    #ä½¿ç”¨`AutoFeatureExtractor.from_pretrained()`å‡½æ•°ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç‰¹å¾æå–å™¨ã€‚
    # `args.model_name_or_path`å‚æ•°æŒ‡å®šäº†é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚
    feature_extractor = AutoFeatureExtractor.from_pretrained(
        args.model_name_or_path)

    # Preprocessing the datasets
    # Define torchvision transforms to be applied to each image.
    if "shortest_edge" in feature_extractor.size:
        size = feature_extractor.size["shortest_edge"]
    else:
        size = (feature_extractor.size["height"],
                feature_extractor.size["width"])

    normalize = Normalize(mean=feature_extractor.image_mean,
                          std=feature_extractor.image_std)
    #`train_transforms`å’Œ`val_transforms`åˆ†åˆ«å®šä¹‰äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è½¬æ¢å™¨ï¼Œ
    # åœ¨è½¬æ¢è¿‡ç¨‹ä¸­åº”ç”¨äº†ä¸€ç³»åˆ—çš„å›¾åƒå¤„ç†æ“ä½œï¼Œå¦‚éšæœºè£å‰ªï¼ˆRandomResizedCropï¼‰ã€éšæœºæ°´å¹³ç¿»è½¬ï¼ˆRandomHorizontalFlipï¼‰ã€è°ƒæ•´å°ºå¯¸ï¼ˆResizeï¼‰ã€ä¸­å¿ƒè£å‰ªï¼ˆCenterCropï¼‰ã€è½¬æ¢ä¸ºå¼ é‡ï¼ˆToTensorï¼‰å’Œå½’ä¸€åŒ–ï¼ˆNormalizeï¼‰ï¼Œ
    # å¹¶ä¿å­˜åˆ°`train_transforms`å’Œ`val_transforms`å˜é‡ä¸­ã€‚
    train_transforms = Compose(
        [
            RandomResizedCrop(size),
            RandomHorizontalFlip(),
            ToTensor(),
            normalize,
        ]
    )
    val_transforms = Compose(
        [
            Resize(size),
            CenterCrop(size),
            ToTensor(),
            normalize,
        ]
    )
    #`preprocess_train()`å’Œ`preprocess_val()`ï¼Œç”¨æ¥åœ¨æ‰¹å¤„ç†è¿‡ç¨‹ä¸­åº”ç”¨ç›¸åº”çš„å›¾åƒè½¬æ¢å™¨ã€‚
    # è¿™äº›å‡½æ•°å°†è¾“å…¥çš„å›¾åƒæ‰¹é‡è¿›è¡Œè½¬æ¢å’Œå¤„ç†ï¼Œå¹¶å°†å¤„ç†åçš„ç»“æœå­˜å‚¨åœ¨`pixel_values`å­—æ®µä¸­ï¼Œä½œä¸ºç‰¹å¾è¾“å…¥å‚ä¸åç»­çš„è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚
    def preprocess_train(example_batch):
        """Apply _train_transforms across a batch."""
        example_batch["pixel_values"] = [train_transforms(
            image.convert("RGB")) for image in example_batch["image"]]
        return example_batch

    def preprocess_val(example_batch):
        """Apply _val_transforms across a batch."""
        example_batch["pixel_values"] = [val_transforms(
            image.convert("RGB")) for image in example_batch["image"]]
        return example_batch

    # Get the datasets: you can either provide your own training and evaluation files (see below)
    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).

    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
    # download the dataset.
    if args.dataset_name is not None:
        # Downloading and loading a dataset from the hub.
        dataset = load_dataset(args.dataset_name, task="image-classification")
    elif args.use_pt_imagefolder:
        # Load a local dataset using a PyTorch Dataset.
        import torchvision.datasets as pt_datasets
        logging.info("Using PyTorch ImageFolder")
        dataset = {
            "train": pt_datasets.ImageFolder(root=args.train_dir, transform=train_transforms),
            "validation": pt_datasets.ImageFolder(root=args.validation_dir, transform=val_transforms),
        }
    else:
        data_files = {}
        if args.train_dir is not None:
            data_files["train"] = os.path.join(args.train_dir, "**")
        if args.validation_dir is not None:
            data_files["validation"] = os.path.join(args.validation_dir, "**")
        dataset = load_dataset(
            "imagefolder",
            data_files=data_files,
            task="image-classification",
        )
        # See more about loading custom images at
        # https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder.

    # If we don't have a validation split, split off a percentage of train as validation.
    args.train_val_split = None if "validation" in dataset.keys() else args.train_val_split
    if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:
        split = dataset["train"].train_test_split(args.train_val_split)
        dataset["train"] = split["train"]
        dataset["validation"] = split["test"]

    # Prepare label mappings.
    # We'll include these in the model's config to get human readable labels in the Inference API.

    if args.use_pt_imagefolder:
        labels = dataset["train"].classes
    else:
        labels = dataset["train"].features["labels"].names

    label2id = {label: str(i) for i, label in enumerate(labels)}
    id2label = {str(i): label for i, label in enumerate(labels)}

    # Load pretrained model and feature extractor
    #
    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
    # download model & vocab.

    if 'CLIP' in args.model_name_or_path:
        config = CLIPVisionConfig.from_pretrained(
            args.model_name_or_path,
            num_labels=len(labels),
            i2label=id2label,
            label2id=label2id,
            finetuning_task="image-classification",
        )
        model = CLIPVisionModel.from_pretrained(
            args.model_name_or_path,
            ignore_mismatched_sizes=args.ignore_mismatched_sizes,
            torch_dtype=torch.float16
        )
        model = CLIPViTForImageClassification(config, model.vision_model)
    elif 'eva' in args.model_name_or_path:
        config = json.load(
            open(os.path.join(args.model_name_or_path, 'config.json')))
        config['num_labels'] = len(labels)
        model = EVAViTForImageClassification(**config)
        state_dict = torch.load(os.path.join(
            args.model_name_or_path, 'pytorch_model.bin'))
        model.load_state_dict(state_dict, strict=False)
    else:
        config = AutoConfig.from_pretrained(
            args.model_name_or_path,
            num_labels=len(labels),
            i2label=id2label,
            label2id=label2id,
            finetuning_task="image-classification",
        )
        model = AutoModelForImageClassification.from_pretrained(
            args.model_name_or_path,
            from_tf=bool(".ckpt" in args.model_name_or_path),
            config=config,
            ignore_mismatched_sizes=args.ignore_mismatched_sizes,
            torch_dtype=torch.float16
        )

    #############################################
    # Teacher-Student model
    model = setup_teacher_student(model, args, accelerator)

    # Setup trainable classification heads
    #è¿™ä¸ªå‡½æ•°ä¼šå¯¹`model`è¿›è¡Œä¸€äº›æ“ä½œï¼Œä½¿å¾—åˆ†ç±»å¤´éƒ¨å¯ä»¥è¢«è®­ç»ƒã€‚
    if args.train_module in ['adapter', 'all']:
        setup_trainable_classification_head(model)
    #é€šè¿‡ä»¥ä¸Šä¸¤æ­¥çš„æ“ä½œï¼Œæ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹è¢«è®¾å®šå¥½ï¼Œå¹¶ä¸”å¯è®­ç»ƒçš„åˆ†ç±»å¤´éƒ¨ä¹Ÿè¢«è®¾ç½®å¥½ï¼Œå‡†å¤‡è¿›è¡Œè®­ç»ƒã€‚
    #############################################

    if args.use_pt_imagefolder:
        train_dataset = dataset["train"]
        eval_dataset = dataset["validation"]

        #`collate_fn`å‡½æ•°æ¥æ”¶ä¸€ä¸ªæ‰¹é‡çš„æ ·æœ¬åˆ—è¡¨ï¼Œå¹¶å°†åƒç´ å€¼å’Œæ ‡ç­¾åˆ†åˆ«å­˜å‚¨åœ¨`pixel_values`å’Œ`labels`ä¸­ï¼Œæœ€åè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ‰¹é‡çš„åƒç´ å€¼å’Œæ ‡ç­¾ã€‚
        def collate_fn(examples):
            pixel_values = torch.stack([example[0] for example in examples])
            labels = torch.tensor([example[1] for example in examples])
            return {"pixel_values": pixel_values, "labels": labels}
    else:
        #ä½¿ç”¨`accelerator.main_process_first()`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿åªæœ‰ä¸»è¿›ç¨‹æ‰§è¡Œä»¥ä¸‹ä»£ç ã€‚
        with accelerator.main_process_first():
            if args.max_train_samples is not None:
                #å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œæ´—ç‰Œå¹¶é€‰æ‹©å‰`args.max_train_samples`ä¸ªæ ·æœ¬ã€‚
                dataset["train"] = dataset["train"].shuffle(
                    seed=args.seed).select(range(args.max_train_samples))
            # Set the training transforms
            #ä½¿ç”¨`preprocess_train`å‡½æ•°å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œè½¬æ¢ã€‚
            train_dataset = dataset["train"].with_transform(preprocess_train)
            #å¦‚æœ`args.max_eval_samples`ä¸ä¸º`None`ï¼Œåˆ™å¯¹éªŒè¯æ•°æ®é›†è¿›è¡Œæ´—ç‰Œå¹¶é€‰æ‹©å‰`args.max_eval_samples`ä¸ªæ ·æœ¬ã€‚
            if args.max_eval_samples is not None:
                dataset["validation"] = dataset["validation"].shuffle(
                    seed=args.seed).select(range(args.max_eval_samples))
            # Set the validation transforms
            #ä½¿ç”¨`preprocess_val`å‡½æ•°å¯¹éªŒè¯æ•°æ®é›†è¿›è¡Œè½¬æ¢ã€‚
            eval_dataset = dataset["validation"].with_transform(preprocess_val)
        #`collate_fn`å‡½æ•°ç”¨äºå°†ä¸€ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼ã€‚
        def collate_fn(examples):
            pixel_values = torch.stack([example["pixel_values"]
                                        for example in examples])
            labels = torch.tensor([example["labels"] for example in examples])
            return {"pixel_values": pixel_values, "labels": labels}

    # DataLoaders creation:

    train_dataloader = DataLoader(
        train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size, num_workers=args.num_workers
    )
    eval_dataloader = DataLoader(
        eval_dataset, collate_fn=collate_fn, batch_size=args.per_device_eval_batch_size, num_workers=args.num_workers
    )

    if args.load_student and not args.restart_training:
        base_results = json.load(
            open(os.path.join(args.load_student, 'all_results.json'), 'r'))
        starting_epoch = base_results['epoch']
        resume_step = base_results['step'] - \
            starting_epoch * len(train_dataloader)
    else:
        starting_epoch = 0
        resume_step = -1

    # Optimizer
    # Split weights in two groups, one with weight decay and the other not.
    #åˆ›å»ºäº†ç”¨äºä¼˜åŒ–æ¨¡å‹å‚æ•°çš„ä¼˜åŒ–å™¨ï¼Œå¹¶å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œåˆ†ç»„è®¾ç½®ã€‚
    no_decay = ["bias", "LayerNorm.weight"]
    # `optimizer_grouped_parameters`æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸ç”¨äºè®¾ç½®ä¸åŒç»„çš„å‚æ•°ã€‚å…¶ä¸­ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®å€¼å¯¹ï¼š
    # â€œparamsâ€ï¼šä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†éœ€è¦ä¼˜åŒ–çš„å‚æ•°ã€‚
    # â€œweight_decayâ€ï¼šä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œè¡¨ç¤ºæƒé‡è¡°å‡çš„ç³»æ•°ã€‚
    optimizer_grouped_parameters = [
        {
            #è®¾ç½®ä¸éœ€è¦æƒé‡è¡°å‡çš„å‚æ•°ï¼Œå³é‚£äº›å‚æ•°åç§°ä¸­ä¸åŒ…å«`no_decay`åˆ—è¡¨ä¸­ä»»ä½•ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä¸”ä¸å±äº`classifier`æ¨¡å—çš„å‚æ•°ã€‚è¿™äº›å‚æ•°å°†åº”ç”¨æƒé‡è¡°å‡ï¼Œç³»æ•°ç”±`args.weight_decay`æŒ‡å®šã€‚
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and "classifier" not in n],
            "weight_decay": args.weight_decay,
        },
        {
            #ç¬¬äºŒä¸ªå­—å…¸ç”¨äºè®¾ç½®éœ€è¦æƒé‡è¡°å‡çš„å‚æ•°ï¼Œå³é‚£äº›å‚æ•°åç§°ä¸­åŒ…å«`no_decay`åˆ—è¡¨ä¸­ä»»ä½•ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä¸”ä¸å±äº`classifier`æ¨¡å—çš„å‚æ•°ã€‚è¿™äº›å‚æ•°å°†ä¸åº”ç”¨æƒé‡è¡°å‡ï¼Œæ‰€ä»¥æƒé‡è¡°å‡ç³»æ•°è®¾ç½®ä¸º0.0ã€‚
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and "classifier" not in n],
            "weight_decay": 0.0,
        },
        {
            #ç¬¬ä¸‰ä¸ªå­—å…¸ç”¨äºè®¾ç½®`classifier`æ¨¡å—ä¸­çš„å‚æ•°ï¼Œä¸è®ºæ˜¯å¦éœ€è¦æƒé‡è¡°å‡ã€‚è¿™äº›å‚æ•°çš„æƒé‡è¡°å‡ç³»æ•°ç”±`args.weight_decay`æŒ‡å®šï¼Œå¹¶ä¸”å­¦ä¹ ç‡è®¾ç½®ä¸º`args.learning_rate * args.classifier_lr_multiplier`ã€‚`classifier_lr_multiplier`æ˜¯ä¸€ä¸ªç”¨äºè°ƒæ•´`classifier`æ¨¡å—å­¦ä¹ ç‡çš„å€æ•°ã€‚
            "params": [p for n, p in model.classifier.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": args.weight_decay,
            "lr": args.learning_rate * args.classifier_lr_multiplier
        },
        {
            #æ ¹æ®`args.optimizer`çš„å€¼é€‰æ‹©ä½¿ç”¨`torch.optim.SGD`è¿˜æ˜¯`torch.optim.AdamW`åˆ›å»ºç›¸åº”çš„ä¼˜åŒ–å™¨å¯¹è±¡ï¼Œå¹¶å°†`optimizer_grouped_parameters`ä½œä¸ºä¼˜åŒ–å™¨çš„å‚æ•°ã€‚å¦‚æœ`args.optimizer`çš„å€¼ä¸æ˜¯"sgd"æˆ–"adamw"ï¼Œåˆ™æŠ›å‡ºä¸€ä¸ªå¼‚å¸¸ã€‚
            "params": [p for n, p in model.classifier.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
            "lr": args.learning_rate * args.classifier_lr_multiplier
        },
    ]
    #æ ¹æ®`args.optimizer`çš„å€¼é€‰æ‹©ä½¿ç”¨`torch.optim.SGD`è¿˜æ˜¯`torch.optim.AdamW`åˆ›å»ºç›¸åº”çš„ä¼˜åŒ–å™¨å¯¹è±¡ï¼Œå¹¶å°†`optimizer_grouped_parameters`ä½œä¸ºä¼˜åŒ–å™¨çš„å‚æ•°ã€‚
    #å¦‚æœ`args.optimizer`çš„å€¼ä¸æ˜¯"sgd"æˆ–"adamw"ï¼Œåˆ™æŠ›å‡ºä¸€ä¸ªå¼‚å¸¸ã€‚
    if args.optimizer == "sgd":
        optimizer = torch.optim.SGD(
            optimizer_grouped_parameters, lr=args.learning_rate, momentum=args.momentum)
    elif args.optimizer == "adamw":
        optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters, lr=args.learning_rate)
    else:
        raise ValueError(f"Unknown optimizer type {args.optimizer}")

    #Scheduler and math around the number of training steps.
    #ç”¨äºè®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆlearning rate schedulerï¼‰ã€‚
    overrode_max_train_steps = False
    #è®¡ç®—äº†æ¯ä¸ªè®­ç»ƒå‘¨æœŸï¼ˆepochï¼‰ä¸­çš„æ›´æ–°æ­¥æ•°ï¼ˆnum_update_steps_per_epochï¼‰ã€‚
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / args.gradient_accumulation_steps)
    #é€šè¿‡åˆ¤æ–­`args.max_train_steps`æ˜¯å¦ä¸ºNoneæ¥ç¡®å®šæ˜¯å¦éœ€è¦è®¡ç®—å¹¶è®¾ç½®`args.max_train_steps`çš„å€¼ã€‚
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    #è°ƒç”¨`get_scheduler`å‡½æ•°æ¥åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    # nameï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„ç±»å‹ã€‚
    # optimizerï¼šä¼˜åŒ–å™¨å¯¹è±¡ï¼Œç”¨äºè®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    # num_warmup_stepsï¼šé¢„çƒ­æ­¥æ•°ï¼Œå³åœ¨è®­ç»ƒå¼€å§‹æ—¶é€æ¸å¢åŠ å­¦ä¹ ç‡çš„æ­¥æ•°ã€‚è®¡ç®—æ–¹å¼æ˜¯`args.num_warmup_steps`ä¹˜ä»¥`args.gradient_accumulation_steps`ã€‚
    # num_training_stepsï¼šæ€»çš„è®­ç»ƒæ­¥æ•°ï¼Œç”¨äºç¡®å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–èŒƒå›´ã€‚è®¡ç®—æ–¹å¼æ˜¯`args.max_train_steps`ä¹˜ä»¥`args.gradient_accumulation_steps`ã€‚
    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )

    # Prepare everything with our `accelerator`.
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
    )

    # We need to recalculate our total training steps as the size of the training dataloader may have changed.
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    # Afterwards we recalculate our number of training epochs
    args.num_train_epochs = math.ceil(
        args.max_train_steps / num_update_steps_per_epoch)

    # Figure out how many steps we should save the Accelerator states
    checkpointing_steps = args.checkpointing_steps
    if checkpointing_steps is not None and checkpointing_steps.isdigit():
        checkpointing_steps = int(checkpointing_steps)

    # We need to initialize the trackers we use, and also store our configuration.
    # The trackers initializes automatically on the main process.
    experiment_config = vars(args)
    # TensorBoard cannot log Enums, need the raw value
    experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
    accelerator.init_trackers("offsite_tuning", experiment_config)

    # Get the metric function
    metric = evaluate.load("accuracy")

    # Train!
    total_batch_size = args.per_device_train_batch_size * \
        accelerator.num_processes * args.gradient_accumulation_steps

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(
        f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
    logger.info(
        f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(
        f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")

    def eval_epoch():
        model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                outputs = model(**batch)
            predictions = outputs.logits.argmax(dim=-1)
            predictions, references = accelerator.gather_for_metrics(
                (predictions, batch["labels"]))
            metric.add_batch(
                predictions=predictions,
                references=references,
            )

        eval_metric = metric.compute()
        return eval_metric["accuracy"]

    if args.select_by_kd:
        teacher_zero_shot_acc = student_zero_shot_acc = 0
        model = to_student(model, args)
    else:
        model = to_teacher(model, args)
        teacher_zero_shot_acc = eval_epoch()

        model = to_student(model, args)
        student_zero_shot_acc = eval_epoch()

    trainable_params = sum(p.numel()
                           for p in model.parameters() if p.requires_grad)

    logger.info(f"Number of trainable parameters: {trainable_params}")

    for name, param in model.named_parameters():
        if param.requires_grad:
            logger.info(
                f"Trainable parameter: {name} with shape {param.shape} and dtype {param.dtype}")

    logger.info(
        f"Teacher zero shot accuracy: {teacher_zero_shot_acc}")
    logger.info(
        f"Student zero shot accuracy: {student_zero_shot_acc}")

    # Only show the progress bar once on each machine.
    progress_bar = tqdm(range(args.max_train_steps),
                        disable=not accelerator.is_local_main_process)
    completed_steps = 0
    starting_epoch = 0

    def evaluator(model):
        if evaluator.eval_steps == 0:
            return

        task_loss = evaluator.interval_task_loss / evaluator.eval_steps
        kd_loss = evaluator.interval_kd_loss / evaluator.eval_steps
        evaluator.interval_task_loss = 0
        evaluator.interval_kd_loss = 0
        evaluator.eval_steps = 0

        if args.select_by_kd:
            is_best = kd_loss < evaluator.best_kd_loss
            evaluator.best_kd_loss = min(evaluator.best_kd_loss, kd_loss)
            eval_acc = plug_acc = 0
        else:
            model = to_teacher(model, args)
            plug_acc = eval_epoch()
            model = to_student(model, args)
            eval_acc = eval_epoch()
            is_best = eval_acc > evaluator.best_acc
            evaluator.best_acc = max(evaluator.best_acc, eval_acc)

        logger.info(
            f"Epoch {epoch} step {completed_steps}: eval_acc: {eval_acc:.4f} plug_acc: {plug_acc:.4f} task_loss: {task_loss:.4f} kd_loss: {kd_loss:.4f}")

        accelerator.log(
            {
                "eval_acc": eval_acc,
                "plug_acc": plug_acc,
                "acc_gap": plug_acc - eval_acc,
                "train_task_loss": task_loss,
                "train_kd_loss": kd_loss,
                "epoch": epoch,
                "step": completed_steps,
            },
            step=completed_steps,
        )
        if not args.no_save_model and is_best and accelerator.is_main_process:
            unwrapped_model = accelerator.unwrap_model(model)
            state_dict = unwrapped_model.student.state_dict()
            for k in state_dict:
                state_dict[k] = state_dict[k].to(torch.float16).cpu()
            torch.save(state_dict, os.path.join(
                args.output_dir, "student.pt"))
            gc.collect()
            torch.cuda.empty_cache()

        if is_best and accelerator.is_main_process:
            with open(os.path.join(args.output_dir, "all_results.json"), "w+") as f:
                json.dump({"best_acc": eval_acc,
                           "plug_acc": plug_acc,
                           "teacher_zero_shot_acc": teacher_zero_shot_acc,
                           "student_zero_shot_acc": student_zero_shot_acc,
                           "train_task_loss": task_loss,
                           "train_kd_loss": kd_loss,
                           "epoch": epoch,
                           "step": completed_steps,
                           "trainable_params": trainable_params}, f)

    evaluator.best_acc = student_zero_shot_acc
    evaluator.best_kd_loss = float("inf")
    evaluator.eval_steps = 0
    evaluator.interval_task_loss = 0
    evaluator.interval_kd_loss = 0

    for epoch in range(starting_epoch, args.num_train_epochs):
        model.train()
        total_task_loss, total_kd_loss = 0, 0
        skipped_steps = 0

        for step, batch in enumerate(train_dataloader):
            # We need to skip steps until we reach the resumed step
            if args.load_student and epoch == starting_epoch and step <= resume_step:
                progress_bar.update(1)
                progress_bar.set_description(
                    f"Skipping step {step} (already completed)")
                completed_steps += 1
                skipped_steps += 1
                continue

            with accelerator.accumulate(model):
                outputs = model(**batch)
                task_loss = outputs.loss

                kd_loss = get_kd_loss(model)

                loss = args.lm_weight * task_loss + args.kd_weight * \
                    kd_loss if args.kd_weight != 0 else task_loss
                progress_bar.set_description(
                    f"Epoch {epoch} - Step {step} - LR: {optimizer.param_groups[0]['lr']:.2e} - Task loss: {task_loss:.4f} - KD loss: {kd_loss:.4f}")

                total_task_loss += task_loss.item()
                total_kd_loss += kd_loss.item()

                evaluator.interval_task_loss += task_loss.item()
                evaluator.interval_kd_loss += kd_loss.item()
                evaluator.eval_steps += 1

                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(
                        model.parameters(), args.max_grad_norm)

                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
            # end accumulate gradients

            # Checks if the accelerator has performed an optimization step behind the scenes
            if accelerator.sync_gradients:
                progress_bar.update(1)
                completed_steps += 1
            else:
                continue

            if completed_steps % args.eval_steps == 0:
                evaluator(model)

        evaluator(model)

    accelerator.end_training()


if __name__ == "__main__":
    main()
